{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "import requests\n",
    "import time\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "google_api_key = \"your_google_api_key_here\"  # Replace with your actual API key\n",
    "\n",
    "async def scrape_website(url, next_button_class, element_to_check):\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url)\n",
    "        all_reviews = []\n",
    "\n",
    "        while True:\n",
    "            await page.wait_for_selector('body')\n",
    "            page_source = await page.content()\n",
    "\n",
    "            # Extract reviews from the current page\n",
    "            reviews = extract_reviews_with_selenium(page_source)\n",
    "            if reviews:\n",
    "                all_reviews.extend(reviews)\n",
    "            else:\n",
    "                print(\"No reviews found on this page.\")\n",
    "\n",
    "            try:\n",
    "                next_button = page.locator(f'.{next_button_class}')\n",
    "                if await next_button.count() > 0:\n",
    "                    await next_button.click()\n",
    "                    await page.wait_for_load_state('networkidle')  # Wait until the next page is loaded\n",
    "                    await page.wait_for_selector(element_to_check)  # Wait for specific element to be visible\n",
    "                else:\n",
    "                    print(\"No more pages.\")\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "                break\n",
    "\n",
    "        await browser.close()\n",
    "        return all_reviews\n",
    "\n",
    "\n",
    "def extract_reviews_with_selenium(html_content):\n",
    "    # Set up Selenium WebDriver with WebDriver Manager\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")  # Run in headless mode\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "    try:\n",
    "        # Load the HTML content in Selenium\n",
    "        driver.get(\"data:text/html;charset=utf-8,\" + html_content)\n",
    "        time.sleep(2)  # Ensure the content loads\n",
    "\n",
    "        # Extract and clean HTML with BeautifulSoup\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        for tag in soup(['script', 'style', 'meta', 'link']):\n",
    "            tag.decompose()\n",
    "\n",
    "        for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):\n",
    "            comment.extract()\n",
    "\n",
    "        cleaned_html = str(soup)\n",
    "\n",
    "        # Use the Google API to extract reviews\n",
    "        response = requests.post(\n",
    "            url=f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={google_api_key}\",\n",
    "            headers={\n",
    "                \"Content-Type\": \"application/json\"\n",
    "            },\n",
    "            json={\n",
    "                \"contents\": [\n",
    "                    {\n",
    "                        \"parts\": [\n",
    "                            {\n",
    "                                'text': \"extract all the reviews in the page and return in json format, if no reviews are found, return NULL \" + cleaned_html\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "        driver.quit()\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            reviews = data['candidates'][0]['content']['parts'][0][\"text\"]\n",
    "            return reviews if reviews != \"NULL\" else []\n",
    "        else:\n",
    "            print(\"Error with Google API:\", response.status_code, response.text)\n",
    "            return []\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        driver.quit()\n",
    "        return []\n",
    "\n",
    "\n",
    "async def main():\n",
    "    all_reviews = await scrape_website(\n",
    "        url='https://2717recovery.com/products/recovery-cream',\n",
    "        next_button_class='jdgm-paginate__next-page',\n",
    "        element_to_check='body'\n",
    "    )\n",
    "\n",
    "    # Save all reviews to a file\n",
    "    with open(\"all_reviews.json\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(all_reviews))\n",
    "\n",
    "    print(\"Reviews extracted:\", all_reviews)\n",
    "\n",
    "\n",
    "asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
